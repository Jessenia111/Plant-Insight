{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1e2462c-3a43-416f-99bb-00b4cf9c032d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:38:42.754871Z",
     "start_time": "2024-12-01T12:38:35.018938Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting coremltools\r\n",
      "  Downloading coremltools-8.1-cp39-none-macosx_11_0_arm64.whl (2.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\r\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Users/mtsenk/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from coremltools) (4.64.1)\r\n",
      "Collecting sympy\r\n",
      "  Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: packaging in /Users/mtsenk/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from coremltools) (22.0)\r\n",
      "Requirement already satisfied: numpy>=1.14.5 in /Users/mtsenk/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from coremltools) (1.23.2)\r\n",
      "Collecting cattrs\r\n",
      "  Downloading cattrs-24.1.2-py3-none-any.whl (66 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pyaml\r\n",
      "  Downloading pyaml-24.9.0-py3-none-any.whl (24 kB)\r\n",
      "Requirement already satisfied: attrs>=21.3.0 in /Users/mtsenk/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from coremltools) (22.1.0)\r\n",
      "Requirement already satisfied: protobuf>=3.1.0 in /Users/mtsenk/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from coremltools) (3.19.4)\r\n",
      "Requirement already satisfied: typing-extensions!=4.6.3,>=4.1.0 in /Users/mtsenk/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from cattrs->coremltools) (4.4.0)\r\n",
      "Collecting exceptiongroup>=1.1.1\r\n",
      "  Downloading exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\r\n",
      "Collecting attrs>=21.3.0\r\n",
      "  Downloading attrs-24.2.0-py3-none-any.whl (63 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /Users/mtsenk/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from pyaml->coremltools) (6.0)\r\n",
      "Collecting mpmath<1.4,>=1.1.0\r\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: mpmath, sympy, pyaml, exceptiongroup, attrs, cattrs, coremltools\r\n",
      "  Attempting uninstall: attrs\r\n",
      "    Found existing installation: attrs 22.1.0\r\n",
      "    Uninstalling attrs-22.1.0:\r\n",
      "      Successfully uninstalled attrs-22.1.0\r\n",
      "Successfully installed attrs-24.2.0 cattrs-24.1.2 coremltools-8.1 exceptiongroup-1.2.2 mpmath-1.3.0 pyaml-24.9.0 sympy-1.13.3\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install tensorflow==2.6.0\n",
    "# %pip install coremltools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cd8aa9-b52d-4a01-ae87-7f1237d4cfa8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T14:10:37.523773Z",
     "start_time": "2024-12-01T14:10:22.972229Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3536 images belonging to 14 classes.\n",
      "Found 1177 images belonging to 14 classes.\n",
      "Found 1184 images belonging to 14 classes.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries:\n",
    "# - TensorFlow: Framework for deep learning.\n",
    "# - MobileNetV2: A pre-trained model for transfer learning.\n",
    "# - ImageDataGenerator: Generates batches of image data with real-time data augmentation.\n",
    "# - Other libraries for numerical operations, class balancing, and model conversion.\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2  # Pre-trained MobileNetV2 model\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D  # Layers for fine-tuning\n",
    "from tensorflow.keras.models import Model  # Functional API for model definition\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator  # Data augmentation and preprocessing\n",
    "import os  # For file and directory operations\n",
    "import numpy as np  # Numerical operations\n",
    "from sklearn.utils.class_weight import compute_class_weight  # Compute class weights for imbalanced data\n",
    "import coremltools as ct  # For CoreML model conversion\n",
    "\n",
    "# Define directories for the dataset:\n",
    "# - `train_dir`: Training data directory.\n",
    "# - `val_dir`: Validation data directory.\n",
    "# - `test_dir`: Testing data directory.\n",
    "# - `model_save_path`: Path to save the trained model in H5 format.\n",
    "# - `mlmodel_save_path`: Path to save the converted CoreML model.\n",
    "\n",
    "train_dir = '/Users/mtsenk/Downloads/train'  # Path to training data\n",
    "val_dir = '/Users/mtsenk/Downloads/validation'  # Path to validation data\n",
    "test_dir = '/Users/mtsenk/Downloads/test'  # Path to testing data\n",
    "model_save_path = '/Users/mtsenk/Downloads/some/trained_model.h5'  # Path to save model (H5 format)\n",
    "mlmodel_save_path = '/Users/mtsenk/Downloads/some/trained_model.mlmodel'  # Path to save CoreML model\n",
    "\n",
    "# Define hyperparameters for the model:\n",
    "# - `img_size`: Dimensions of input images (224x224 for MobileNetV2).\n",
    "# - `batch_size`: Number of images per training batch.\n",
    "# - `epochs`: Total number of training iterations over the dataset.\n",
    "# - `learning_rate`: Step size for weight updates during training.\n",
    "\n",
    "img_size = 224  # Image size for resizing\n",
    "batch_size = 32  # Number of images per batch\n",
    "epochs = 95  # Number of training epochs\n",
    "learning_rate = 0.0001  # Initial learning rate\n",
    "\n",
    "# Create an ImageDataGenerator instance for training:\n",
    "# - `rescale`: Normalize pixel values to range [0, 1].\n",
    "# - Other parameters: Augmentations like rotation, shifting, zooming, flipping, etc.\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,  # Normalize pixel values\n",
    "    rotation_range=30,  # Randomly rotate images by up to 30 degrees\n",
    "    width_shift_range=0.2,  # Randomly shift images horizontally\n",
    "    height_shift_range=0.2,  # Randomly shift images vertically\n",
    "    shear_range=0.2,  # Apply shearing transformations\n",
    "    zoom_range=0.2,  # Randomly zoom into images\n",
    "    horizontal_flip=True,  # Flip images horizontally\n",
    "    fill_mode=\"nearest\"  # Fill empty pixels with the nearest pixel value\n",
    ")\n",
    "\n",
    "# Create an ImageDataGenerator instance for validation and testing:\n",
    "# - Only normalization is applied to avoid modifying validation/test data.\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1.0/255)  # Normalize pixel values for validation\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255)  # Normalize pixel values for testing\n",
    "\n",
    "# Create data generators for training, validation, and testing:\n",
    "# - `flow_from_directory`: Reads images from the specified directory and applies transformations.\n",
    "# - `target_size`: Resizes images to (img_size, img_size).\n",
    "# - `batch_size`: Number of images per batch.\n",
    "# - `class_mode`: Indicates categorical classification.\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,  # Training data directory\n",
    "    target_size=(img_size, img_size),  # Resize images\n",
    "    batch_size=batch_size,  # Batch size\n",
    "    class_mode='categorical'  # Categorical classification\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,  # Validation data directory\n",
    "    target_size=(img_size, img_size),  # Resize images\n",
    "    batch_size=batch_size,  # Batch size\n",
    "    class_mode='categorical'  # Categorical classification\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,  # Testing data directory\n",
    "    target_size=(img_size, img_size),  # Resize images\n",
    "    batch_size=batch_size,  # Batch size\n",
    "    class_mode='categorical'  # Categorical classification\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f551e76-5ee3-4f6c-b5e5-da08fabb6ffb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T14:10:45.642943Z",
     "start_time": "2024-12-01T14:10:42.490756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "9406464/9406464 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# we use tensorflow pre-trained model\n",
    "base_model = MobileNetV2(input_shape=(img_size, img_size, 3), include_top=False, weights='imagenet')\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88955413-0f08-4c68-ab30-fb2048db287d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T14:10:48.700069Z",
     "start_time": "2024-12-01T14:10:48.587628Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add additional layers on top of the pre-trained base model:\n",
    "# - `base_model.output`: Output from the pre-trained base model.\n",
    "# - `GlobalAveragePooling2D`: Reduces the feature map size to a single vector by averaging, which helps prevent overfitting.\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)  # Apply global average pooling to the output of the base model\n",
    "\n",
    "# Add a fully connected (dense) layer with 128 units and ReLU activation for learning complex features.\n",
    "x = Dense(128, activation='relu')(x)  # Dense layer with ReLU activation\n",
    "\n",
    "# Add a dropout layer to randomly set 50% of the neurons to zero during training, reducing overfitting.\n",
    "x = Dropout(0.5)(x)  # Dropout layer to reduce overfitting\n",
    "\n",
    "# Add the final output layer:\n",
    "# - The number of neurons is equal to the number of classes (`train_generator.num_classes`).\n",
    "# - The softmax activation function is used for multi-class classification.\n",
    "output = Dense(train_generator.num_classes, activation='softmax')(x)  # Output layer for classification\n",
    "\n",
    "# Define the final model by connecting the input of the base model to the output layer.\n",
    "model = Model(inputs=base_model.input, outputs=output)  # Create the complete model\n",
    "\n",
    "# Compile the model to specify:\n",
    "# - Optimizer: Adam optimizer with a predefined learning rate.\n",
    "# - Loss function: Categorical cross-entropy for multi-class classification.\n",
    "# - Evaluation metric: Accuracy to measure the performance of the model.\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),  # Adam optimizer with a specified learning rate\n",
    "    loss='categorical_crossentropy',  # Loss function for multi-class classification\n",
    "    metrics=['accuracy']  # Metric to evaluate during training and validation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5ba907-7033-4f03-a969-a154bd99f4f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T17:18:30.265156Z",
     "start_time": "2024-12-01T14:10:50.858231Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/95\n",
      " 61/111 [===============>..............] - ETA: 39s - loss: 2.8069 - accuracy: 0.1112"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mtsenk/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111/111 [==============================] - 120s 1s/step - loss: 2.6280 - accuracy: 0.1581 - val_loss: 2.2029 - val_accuracy: 0.3475\n",
      "Epoch 2/95\n",
      "111/111 [==============================] - 118s 1s/step - loss: 2.1262 - accuracy: 0.3111 - val_loss: 1.8017 - val_accuracy: 0.4894\n",
      "Epoch 3/95\n",
      "111/111 [==============================] - 120s 1s/step - loss: 1.8108 - accuracy: 0.4219 - val_loss: 1.5069 - val_accuracy: 0.5820\n",
      "Epoch 4/95\n",
      "111/111 [==============================] - 118s 1s/step - loss: 1.5450 - accuracy: 0.5040 - val_loss: 1.2908 - val_accuracy: 0.6296\n",
      "Epoch 5/95\n",
      "111/111 [==============================] - 121s 1s/step - loss: 1.3667 - accuracy: 0.5625 - val_loss: 1.1534 - val_accuracy: 0.6703\n",
      "Epoch 6/95\n",
      "111/111 [==============================] - 121s 1s/step - loss: 1.2720 - accuracy: 0.5902 - val_loss: 1.0736 - val_accuracy: 0.6907\n",
      "Epoch 7/95\n",
      "111/111 [==============================] - 113s 1s/step - loss: 1.1632 - accuracy: 0.6380 - val_loss: 0.9856 - val_accuracy: 0.7052\n",
      "Epoch 8/95\n",
      "111/111 [==============================] - 112s 1s/step - loss: 1.0916 - accuracy: 0.6561 - val_loss: 0.9397 - val_accuracy: 0.7137\n",
      "Epoch 9/95\n",
      "111/111 [==============================] - 112s 1s/step - loss: 1.0410 - accuracy: 0.6725 - val_loss: 0.9036 - val_accuracy: 0.7196\n",
      "Epoch 10/95\n",
      "111/111 [==============================] - 112s 1s/step - loss: 0.9885 - accuracy: 0.6830 - val_loss: 0.8539 - val_accuracy: 0.7375\n",
      "Epoch 11/95\n",
      "111/111 [==============================] - 112s 1s/step - loss: 0.9229 - accuracy: 0.7090 - val_loss: 0.8186 - val_accuracy: 0.7443\n",
      "Epoch 12/95\n",
      "111/111 [==============================] - 114s 1s/step - loss: 0.9273 - accuracy: 0.7064 - val_loss: 0.8008 - val_accuracy: 0.7528\n",
      "Epoch 13/95\n",
      "111/111 [==============================] - 113s 1s/step - loss: 0.8487 - accuracy: 0.7353 - val_loss: 0.7723 - val_accuracy: 0.7579\n",
      "Epoch 14/95\n",
      "111/111 [==============================] - 114s 1s/step - loss: 0.8474 - accuracy: 0.7234 - val_loss: 0.7569 - val_accuracy: 0.7587\n",
      "Epoch 15/95\n",
      "111/111 [==============================] - 115s 1s/step - loss: 0.8070 - accuracy: 0.7384 - val_loss: 0.7238 - val_accuracy: 0.7757\n",
      "Epoch 16/95\n",
      "111/111 [==============================] - 114s 1s/step - loss: 0.7825 - accuracy: 0.7452 - val_loss: 0.7129 - val_accuracy: 0.7766\n",
      "Epoch 17/95\n",
      "111/111 [==============================] - 116s 1s/step - loss: 0.7663 - accuracy: 0.7647 - val_loss: 0.7136 - val_accuracy: 0.7638\n",
      "Epoch 18/95\n",
      "111/111 [==============================] - 119s 1s/step - loss: 0.7300 - accuracy: 0.7636 - val_loss: 0.6847 - val_accuracy: 0.7740\n",
      "Epoch 19/95\n",
      "111/111 [==============================] - 117s 1s/step - loss: 0.7358 - accuracy: 0.7633 - val_loss: 0.6682 - val_accuracy: 0.7825\n",
      "Epoch 20/95\n",
      "111/111 [==============================] - 117s 1s/step - loss: 0.7057 - accuracy: 0.7771 - val_loss: 0.6579 - val_accuracy: 0.7850\n",
      "Epoch 21/95\n",
      "111/111 [==============================] - 118s 1s/step - loss: 0.6912 - accuracy: 0.7735 - val_loss: 0.6464 - val_accuracy: 0.7884\n",
      "Epoch 22/95\n",
      "111/111 [==============================] - 117s 1s/step - loss: 0.6700 - accuracy: 0.7848 - val_loss: 0.6354 - val_accuracy: 0.7995\n",
      "Epoch 23/95\n",
      "111/111 [==============================] - 118s 1s/step - loss: 0.6467 - accuracy: 0.7936 - val_loss: 0.6391 - val_accuracy: 0.7901\n",
      "Epoch 24/95\n",
      "111/111 [==============================] - 118s 1s/step - loss: 0.6217 - accuracy: 0.8009 - val_loss: 0.6086 - val_accuracy: 0.7961\n",
      "Epoch 25/95\n",
      "111/111 [==============================] - 116s 1s/step - loss: 0.6317 - accuracy: 0.7981 - val_loss: 0.6086 - val_accuracy: 0.7910\n",
      "Epoch 26/95\n",
      "111/111 [==============================] - 117s 1s/step - loss: 0.6122 - accuracy: 0.8063 - val_loss: 0.6133 - val_accuracy: 0.8054\n",
      "Epoch 27/95\n",
      "111/111 [==============================] - 117s 1s/step - loss: 0.5865 - accuracy: 0.8094 - val_loss: 0.6118 - val_accuracy: 0.7969\n",
      "Epoch 28/95\n",
      "111/111 [==============================] - 116s 1s/step - loss: 0.5837 - accuracy: 0.8080 - val_loss: 0.5914 - val_accuracy: 0.8012\n",
      "Epoch 29/95\n",
      "111/111 [==============================] - 116s 1s/step - loss: 0.5611 - accuracy: 0.8204 - val_loss: 0.5774 - val_accuracy: 0.8080\n",
      "Epoch 30/95\n",
      "111/111 [==============================] - 115s 1s/step - loss: 0.5690 - accuracy: 0.8145 - val_loss: 0.5795 - val_accuracy: 0.8088\n",
      "Epoch 31/95\n",
      "111/111 [==============================] - 120s 1s/step - loss: 0.5656 - accuracy: 0.8187 - val_loss: 0.5849 - val_accuracy: 0.8046\n",
      "Epoch 32/95\n",
      "111/111 [==============================] - 118s 1s/step - loss: 0.5445 - accuracy: 0.8266 - val_loss: 0.5691 - val_accuracy: 0.8156\n",
      "Epoch 33/95\n",
      "111/111 [==============================] - 121s 1s/step - loss: 0.5351 - accuracy: 0.8244 - val_loss: 0.5767 - val_accuracy: 0.8046\n",
      "Epoch 34/95\n",
      "111/111 [==============================] - 119s 1s/step - loss: 0.5224 - accuracy: 0.8275 - val_loss: 0.5551 - val_accuracy: 0.8114\n",
      "Epoch 35/95\n",
      "111/111 [==============================] - 118s 1s/step - loss: 0.5362 - accuracy: 0.8281 - val_loss: 0.5688 - val_accuracy: 0.8037\n",
      "Epoch 36/95\n",
      "111/111 [==============================] - 121s 1s/step - loss: 0.5200 - accuracy: 0.8303 - val_loss: 0.5527 - val_accuracy: 0.8165\n",
      "Epoch 37/95\n",
      "111/111 [==============================] - 117s 1s/step - loss: 0.5148 - accuracy: 0.8283 - val_loss: 0.5466 - val_accuracy: 0.8173\n",
      "Epoch 38/95\n",
      "111/111 [==============================] - 116s 1s/step - loss: 0.5083 - accuracy: 0.8377 - val_loss: 0.5407 - val_accuracy: 0.8156\n",
      "Epoch 39/95\n",
      "111/111 [==============================] - 116s 1s/step - loss: 0.4818 - accuracy: 0.8413 - val_loss: 0.5428 - val_accuracy: 0.8284\n",
      "Epoch 40/95\n",
      "111/111 [==============================] - 116s 1s/step - loss: 0.5008 - accuracy: 0.8363 - val_loss: 0.5439 - val_accuracy: 0.8122\n",
      "Epoch 41/95\n",
      "111/111 [==============================] - 117s 1s/step - loss: 0.4806 - accuracy: 0.8467 - val_loss: 0.5333 - val_accuracy: 0.8190\n",
      "Epoch 42/95\n",
      "111/111 [==============================] - 116s 1s/step - loss: 0.4808 - accuracy: 0.8464 - val_loss: 0.5331 - val_accuracy: 0.8173\n",
      "Epoch 43/95\n",
      "111/111 [==============================] - 120s 1s/step - loss: 0.4601 - accuracy: 0.8464 - val_loss: 0.5311 - val_accuracy: 0.8173\n",
      "Epoch 44/95\n",
      "111/111 [==============================] - 118s 1s/step - loss: 0.4576 - accuracy: 0.8470 - val_loss: 0.5292 - val_accuracy: 0.8182\n",
      "Epoch 45/95\n",
      "111/111 [==============================] - 118s 1s/step - loss: 0.4563 - accuracy: 0.8518 - val_loss: 0.5177 - val_accuracy: 0.8258\n",
      "Epoch 46/95\n",
      "111/111 [==============================] - 120s 1s/step - loss: 0.4569 - accuracy: 0.8549 - val_loss: 0.5240 - val_accuracy: 0.8199\n",
      "Epoch 47/95\n",
      "111/111 [==============================] - 120s 1s/step - loss: 0.4565 - accuracy: 0.8501 - val_loss: 0.5215 - val_accuracy: 0.8241\n",
      "Epoch 48/95\n",
      "111/111 [==============================] - 118s 1s/step - loss: 0.4210 - accuracy: 0.8626 - val_loss: 0.5178 - val_accuracy: 0.8207\n",
      "Epoch 49/95\n",
      "111/111 [==============================] - 120s 1s/step - loss: 0.4306 - accuracy: 0.8577 - val_loss: 0.5177 - val_accuracy: 0.8241\n",
      "Epoch 50/95\n",
      "111/111 [==============================] - 120s 1s/step - loss: 0.4276 - accuracy: 0.8631 - val_loss: 0.5197 - val_accuracy: 0.8301\n",
      "Epoch 51/95\n",
      "111/111 [==============================] - 119s 1s/step - loss: 0.4291 - accuracy: 0.8586 - val_loss: 0.5101 - val_accuracy: 0.8250\n",
      "Epoch 52/95\n",
      "111/111 [==============================] - 119s 1s/step - loss: 0.4461 - accuracy: 0.8561 - val_loss: 0.5108 - val_accuracy: 0.8267\n",
      "Epoch 53/95\n",
      "111/111 [==============================] - 118s 1s/step - loss: 0.4120 - accuracy: 0.8671 - val_loss: 0.5070 - val_accuracy: 0.8326\n",
      "Epoch 54/95\n",
      "111/111 [==============================] - 119s 1s/step - loss: 0.4093 - accuracy: 0.8657 - val_loss: 0.4963 - val_accuracy: 0.8326\n",
      "Epoch 55/95\n",
      "111/111 [==============================] - 119s 1s/step - loss: 0.4008 - accuracy: 0.8725 - val_loss: 0.5196 - val_accuracy: 0.8182\n",
      "Epoch 56/95\n",
      "111/111 [==============================] - 119s 1s/step - loss: 0.4020 - accuracy: 0.8702 - val_loss: 0.5081 - val_accuracy: 0.8301\n",
      "Epoch 57/95\n",
      "111/111 [==============================] - 119s 1s/step - loss: 0.3965 - accuracy: 0.8727 - val_loss: 0.4980 - val_accuracy: 0.8318\n",
      "Epoch 58/95\n",
      "111/111 [==============================] - 119s 1s/step - loss: 0.3899 - accuracy: 0.8719 - val_loss: 0.4962 - val_accuracy: 0.8352\n",
      "Epoch 59/95\n",
      "111/111 [==============================] - 119s 1s/step - loss: 0.3892 - accuracy: 0.8742 - val_loss: 0.5033 - val_accuracy: 0.8292\n",
      "Epoch 60/95\n",
      "111/111 [==============================] - 118s 1s/step - loss: 0.3848 - accuracy: 0.8767 - val_loss: 0.5067 - val_accuracy: 0.8267\n",
      "Epoch 61/95\n",
      "111/111 [==============================] - 121s 1s/step - loss: 0.3800 - accuracy: 0.8792 - val_loss: 0.4949 - val_accuracy: 0.8318\n",
      "Epoch 62/95\n",
      "111/111 [==============================] - 119s 1s/step - loss: 0.3792 - accuracy: 0.8775 - val_loss: 0.4938 - val_accuracy: 0.8284\n",
      "Epoch 63/95\n",
      "111/111 [==============================] - 120s 1s/step - loss: 0.3729 - accuracy: 0.8798 - val_loss: 0.5003 - val_accuracy: 0.8216\n",
      "Epoch 64/95\n",
      "111/111 [==============================] - 120s 1s/step - loss: 0.3546 - accuracy: 0.8821 - val_loss: 0.5053 - val_accuracy: 0.8267\n",
      "Epoch 65/95\n",
      "111/111 [==============================] - 120s 1s/step - loss: 0.3700 - accuracy: 0.8787 - val_loss: 0.4849 - val_accuracy: 0.8326\n",
      "Epoch 66/95\n",
      "111/111 [==============================] - 120s 1s/step - loss: 0.3588 - accuracy: 0.8784 - val_loss: 0.4991 - val_accuracy: 0.8292\n",
      "Epoch 67/95\n",
      "111/111 [==============================] - 120s 1s/step - loss: 0.3541 - accuracy: 0.8852 - val_loss: 0.5029 - val_accuracy: 0.8301\n",
      "Epoch 68/95\n",
      "111/111 [==============================] - 120s 1s/step - loss: 0.3603 - accuracy: 0.8804 - val_loss: 0.4858 - val_accuracy: 0.8394\n",
      "Epoch 69/95\n",
      "111/111 [==============================] - 120s 1s/step - loss: 0.3396 - accuracy: 0.8903 - val_loss: 0.4778 - val_accuracy: 0.8360\n",
      "Epoch 70/95\n",
      "111/111 [==============================] - 120s 1s/step - loss: 0.3411 - accuracy: 0.8863 - val_loss: 0.4937 - val_accuracy: 0.8275\n",
      "Epoch 71/95\n",
      "111/111 [==============================] - 120s 1s/step - loss: 0.3262 - accuracy: 0.8903 - val_loss: 0.4865 - val_accuracy: 0.8267\n",
      "Epoch 72/95\n",
      "111/111 [==============================] - 120s 1s/step - loss: 0.3292 - accuracy: 0.8923 - val_loss: 0.4916 - val_accuracy: 0.8352\n",
      "Epoch 73/95\n",
      "111/111 [==============================] - 120s 1s/step - loss: 0.3242 - accuracy: 0.8956 - val_loss: 0.4882 - val_accuracy: 0.8309\n",
      "Epoch 74/95\n",
      "111/111 [==============================] - 121s 1s/step - loss: 0.3319 - accuracy: 0.8891 - val_loss: 0.4767 - val_accuracy: 0.8428\n",
      "Epoch 75/95\n",
      "111/111 [==============================] - 120s 1s/step - loss: 0.3370 - accuracy: 0.8886 - val_loss: 0.4710 - val_accuracy: 0.8360\n",
      "Epoch 76/95\n",
      "111/111 [==============================] - 120s 1s/step - loss: 0.3358 - accuracy: 0.8891 - val_loss: 0.4752 - val_accuracy: 0.8445\n",
      "Epoch 77/95\n",
      "111/111 [==============================] - 120s 1s/step - loss: 0.3460 - accuracy: 0.8883 - val_loss: 0.4854 - val_accuracy: 0.8318\n",
      "Epoch 78/95\n",
      "111/111 [==============================] - 120s 1s/step - loss: 0.3364 - accuracy: 0.8886 - val_loss: 0.4900 - val_accuracy: 0.8335\n",
      "Epoch 79/95\n",
      "111/111 [==============================] - 120s 1s/step - loss: 0.3277 - accuracy: 0.8900 - val_loss: 0.4873 - val_accuracy: 0.8343\n",
      "Epoch 80/95\n",
      "111/111 [==============================] - 120s 1s/step - loss: 0.3103 - accuracy: 0.8985 - val_loss: 0.4831 - val_accuracy: 0.8437\n",
      "Epoch 81/95\n",
      "111/111 [==============================] - 120s 1s/step - loss: 0.3081 - accuracy: 0.8990 - val_loss: 0.4787 - val_accuracy: 0.8403\n",
      "Epoch 82/95\n",
      "111/111 [==============================] - 120s 1s/step - loss: 0.2953 - accuracy: 0.9053 - val_loss: 0.4813 - val_accuracy: 0.8377\n",
      "Epoch 83/95\n",
      "111/111 [==============================] - 120s 1s/step - loss: 0.3070 - accuracy: 0.8939 - val_loss: 0.4747 - val_accuracy: 0.8428\n",
      "Epoch 84/95\n",
      "111/111 [==============================] - 120s 1s/step - loss: 0.3129 - accuracy: 0.9016 - val_loss: 0.4766 - val_accuracy: 0.8411\n",
      "Epoch 85/95\n",
      "111/111 [==============================] - 120s 1s/step - loss: 0.3052 - accuracy: 0.8979 - val_loss: 0.4753 - val_accuracy: 0.8352\n",
      "Epoch 86/95\n",
      "111/111 [==============================] - 120s 1s/step - loss: 0.3036 - accuracy: 0.9013 - val_loss: 0.4874 - val_accuracy: 0.8335\n",
      "Epoch 87/95\n",
      "111/111 [==============================] - 120s 1s/step - loss: 0.3087 - accuracy: 0.8965 - val_loss: 0.4892 - val_accuracy: 0.8301\n",
      "Epoch 88/95\n",
      "111/111 [==============================] - 120s 1s/step - loss: 0.2965 - accuracy: 0.9072 - val_loss: 0.4847 - val_accuracy: 0.8275\n",
      "Epoch 89/95\n",
      "111/111 [==============================] - 135s 1s/step - loss: 0.2943 - accuracy: 0.9024 - val_loss: 0.4836 - val_accuracy: 0.8335\n",
      "Epoch 90/95\n",
      "111/111 [==============================] - 120s 1s/step - loss: 0.3101 - accuracy: 0.8965 - val_loss: 0.4794 - val_accuracy: 0.8292\n",
      "Epoch 91/95\n",
      "111/111 [==============================] - 120s 1s/step - loss: 0.2875 - accuracy: 0.9104 - val_loss: 0.4689 - val_accuracy: 0.8403\n",
      "Epoch 92/95\n",
      "111/111 [==============================] - 119s 1s/step - loss: 0.2956 - accuracy: 0.8993 - val_loss: 0.4747 - val_accuracy: 0.8377\n",
      "Epoch 93/95\n",
      "111/111 [==============================] - 119s 1s/step - loss: 0.2866 - accuracy: 0.9055 - val_loss: 0.4784 - val_accuracy: 0.8386\n",
      "Epoch 94/95\n",
      "111/111 [==============================] - 119s 1s/step - loss: 0.3018 - accuracy: 0.9027 - val_loss: 0.4752 - val_accuracy: 0.8420\n",
      "Epoch 95/95\n",
      "111/111 [==============================] - 119s 1s/step - loss: 0.2742 - accuracy: 0.9118 - val_loss: 0.4617 - val_accuracy: 0.8428\n",
      "Model saved to /Users/mtsenk/Downloads/some/trained_model.h5\n"
     ]
    }
   ],
   "source": [
    "# Train the model using the training data generator:\n",
    "# - `train_generator`: Provides batches of training images and labels.\n",
    "# - `epochs`: Number of complete passes through the training dataset.\n",
    "# - `validation_data`: Data generator for validation to monitor model performance.\n",
    "history = model.fit(\n",
    "    train_generator,  # Training data generator\n",
    "    epochs=epochs,  # Number of epochs\n",
    "    validation_data=val_generator  # Validation data generator\n",
    ")\n",
    "\n",
    "# Save the trained model to the specified file path:\n",
    "# - Saves the model in H5 format, which includes the model architecture, weights, and optimizer state.\n",
    "model.save(model_save_path)  # Save the trained model to the specified path\n",
    "print(f\"Model saved to {model_save_path}\")  # Print confirmation of saved model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c4ffe8b-bdee-46cb-90b9-911eb5e06fc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T17:37:27.606153Z",
     "start_time": "2024-12-01T17:25:11.565704Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/100\n",
      "111/111 [==============================] - 135s 1s/step - loss: 0.9597 - accuracy: 0.6937 - val_loss: 0.4528 - val_accuracy: 0.8437\n",
      "Epoch 96/100\n",
      "111/111 [==============================] - 121s 1s/step - loss: 0.7176 - accuracy: 0.7653 - val_loss: 0.4481 - val_accuracy: 0.8530\n",
      "Epoch 97/100\n",
      "111/111 [==============================] - 120s 1s/step - loss: 0.6282 - accuracy: 0.7921 - val_loss: 0.4356 - val_accuracy: 0.8590\n",
      "Epoch 98/100\n",
      "111/111 [==============================] - 119s 1s/step - loss: 0.5504 - accuracy: 0.8190 - val_loss: 0.4285 - val_accuracy: 0.8666\n",
      "Epoch 99/100\n",
      "111/111 [==============================] - 120s 1s/step - loss: 0.5200 - accuracy: 0.8224 - val_loss: 0.4143 - val_accuracy: 0.8632\n",
      "Epoch 100/100\n",
      "111/111 [==============================] - 121s 1s/step - loss: 0.4779 - accuracy: 0.8380 - val_loss: 0.4085 - val_accuracy: 0.8658\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning\n",
    "base_model.trainable = True\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate / 10),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_fine = model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs + 5,\n",
    "    initial_epoch=history.epoch[-1],\n",
    "    validation_data=val_generator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b72f2b0-e236-4512-9692-64942622868d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T17:38:03.085004Z",
     "start_time": "2024-12-01T17:37:37.759249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 25s 658ms/step - loss: 0.4294 - accuracy: 0.8505\n",
      "Test Accuracy: 0.8505067825317383\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {test_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136bd2efdaf775f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T17:50:04.035296Z",
     "start_time": "2024-12-01T17:49:38.811274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/37 [=============>................] - ETA: 12s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mtsenk/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 25s 663ms/step\n",
      "Precision: 0.07601351351351351\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score  # Import function to compute precision metric\n",
    "\n",
    "# Predict class probabilities for the test data using the trained model:\n",
    "# - `test_generator`: Provides test data in batches.\n",
    "# - Returns probabilities for each class.\n",
    "y_pred_probs = model.predict(test_generator)\n",
    "\n",
    "# Convert predicted probabilities to class indices:\n",
    "# - `np.argmax`: Gets the index of the highest probability for each prediction.\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Get true class labels from the test data:\n",
    "# - `test_generator.classes`: Returns true labels for the test data.\n",
    "y_true = test_generator.classes\n",
    "\n",
    "# Calculate the precision score:\n",
    "# - Compares predicted classes (`y_pred`) with true classes (`y_true`).\n",
    "# - `average='micro'`: Calculates precision globally by counting total true positives and false positives.\n",
    "precision = precision_score(y_true, y_pred, average='micro')\n",
    "\n",
    "# Print the calculated precision:\n",
    "print(f\"Precision: {precision}\")  # Displays the precision metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b329d7-d28b-4267-8d0b-ab7b4c1c6bfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T17:52:50.224038Z",
     "start_time": "2024-12-01T17:52:45.548264Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running TensorFlow Graph Passes: 100%|██████████| 6/6 [00:00<00:00, 19.00 passes/s]\n",
      "Converting TF Frontend ==> MIL Ops: 100%|██████████| 431/431 [00:00<00:00, 3591.10 ops/s]\n",
      "Running MIL frontend_tensorflow2 pipeline: 100%|██████████| 7/7 [00:00<00:00, 77.63 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 89/89 [00:01<00:00, 68.19 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 230.03 passes/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core ML model saved to /Users/mtsenk/Downloads/some/trained_model.mlpackage\n"
     ]
    }
   ],
   "source": [
    "# converting to ml model for usage in swift app\n",
    "mlmodel = ct.convert(\n",
    "    model,\n",
    "    inputs=[ct.ImageType(shape=(1, img_size, img_size, 3), scale=1.0/255)],\n",
    "    minimum_deployment_target=ct.target.iOS15\n",
    ")\n",
    "mlmodel.save(mlmodel_save_path)\n",
    "print(f\"Core ML model saved to {mlmodel_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d81f44-08b2-48b5-8707-4be3dac31683",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
